# -*- coding: utf-8 -*-
"""CNN for Binary Image Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MBa8l2hVPytmUMvVE58hsRfgWMwRohB-

# Step 1: Installation and setup
"""

!pip install --upgrade pip setuptools

!pip install tensorflow

import tensorflow as tf

print(tf.__version__)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""# Step 2: Importing the dataset from kaggle to Colab"""

# Installing Kaggle API
! pip install -q kaggle

# Create a directory as kaggle
! mkdir -p ~/.kaggle

# Import API key to google colab
from google.colab import files
uploaded = files.upload()

# copy API key to kaggle directory
! cp kaggle.json ~/.kaggle/

# Disable API key
! chmod 600 /root/.kaggle/kaggle.json

# list of datasets
! kaggle datasets list

# Importing the dataset

! kaggle datasets download -d tongpython/cat-and-dog

# To use the dataset we have to unzip the dataset

# unzipping the dataset

! unzip -q /content/cat-and-dog.zip

"""# Step 3: Building the Model"""

# Creating an object (Initializing CNN)
model = tf.keras.models.Sequential()

# Adding first CNN layer (input layer)
# 1) filters = 64
# 2) kernal size = 3
# 3) padding = same
# 4) activation = ReLU
# 5) input shape = (32,32,3)

# The image we have download from kaggle.com has more dimension 32 *32 *3 but we are reducing its size
# so that the processing is faster

model.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3, padding='same', activation='relu', input_shape = [32,32,3]))

# Adding maxpool layer
# 1) pool size = 2 (size of the pixels on which we have to select the features)
# 2) strides = 2 ( moving window)
# 3) padding = valid

model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=2, padding = 'valid'))

# Adding second CNN layer and maxpool layer

model.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3, padding='same', activation='relu'))

model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=2, padding = 'valid'))

# Converting arrays into a single vector using flattening layer

model.add(tf.keras.layers.Flatten())

# Adding the dropout layer
# Dropout is a regularization technique where randomly selected neurons are ignored during the training process
# Dropout layer will prevent our model from learning too much from our data

model.add(tf.keras.layers.Dropout(0.4))

# adding fully connected layer

model.add(tf.keras.layers.Dense(units=128, activation='relu'))

# Adding output layer

# we have binary output so sigmoid function is used

model.add(tf.keras.layers.Dense(units=1, activation ='sigmoid'))

# Compiling the model

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

"""# Step 4: Fitting CNN to images"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

training_data_dir = '/content/training_set/training_set'
test_data_dir = '/content/test_set/test_set'

# Rescaling Images

datagen = ImageDataGenerator(rescale= 1./255)

training_set = datagen.flow_from_directory(directory=training_data_dir, target_size=(32,32), classes=['dogs', 'cats'],
                                           class_mode = 'binary', batch_size= 20)
test_set = datagen.flow_from_directory(directory=test_data_dir, target_size=(32,32), classes=['dogs', 'cats'],
                                           class_mode = 'binary', batch_size= 20)

len(training_set), len(test_set)

len(training_set)* 20, len(test_set)* 20

"""This is because we have take a batch size 20"""

test_set.batch_size

history = model.fit_generator(generator=training_set, steps_per_epoch=401, epochs=20, validation_data=test_set, validation_steps=102)

"""# Step 5: plotting the learning curve"""

def learning_curve(history, epoch):

  # training vs validation accuracy
  epoch_range = range(1, epoch+1)
  plt.plot(epoch_range, history.history['accuracy'])
  plt.plot(epoch_range, history.history['val_accuracy'])
  plt.title('Model Accuracy')
  plt.ylabel('Accuracy')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'val'], loc='upper left')
  plt.show()

  # training vs validation loss
  plt.plot(epoch_range, history.history['loss'])
  plt.plot(epoch_range, history.history['val_loss'])
  plt.title('Model loss')
  plt.ylabel('loss')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'val'], loc='upper left')
  plt.show()

learning_curve(history, 20)

"""Model is overfitted after 5th epoch

Tarining loss is decraesing but validation loss is not decreasing continuously
"""



